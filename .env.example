# LLM
BASE_URL=
API_KEY=
MODEL=
LLM_SEMAPHORE=10

# VLM
VLM_BASE_URL=
VLM_API_KEY=
VLM_MODEL=
VLM_SEMAPHORE=10

# LLM for benchmark purpose, using LLM to judge the output
# You can use the same endpoint as the one used for RAGondin
JUDGE_BASE_URL=
JUDGE_MODEL=
JUDGE_API_KEY=

# App
APP_URL=localhost
APP_PORT=8080 # this is the forwarded port
API_NUM_WORKERS=1 # Number of uvicorn workers for the FastAPI app

# VLLM
VLLM_PORT=8007

# Vector db VDB Milvus
VDB_HOST=milvus
VDB_PORT=19530
VDB_CONNECTOR_NAME=milvus

# RETRIEVER
CONTEXTUAL_RETRIEVAL=true
RETRIEVER_TOP_K=20 # Number of documents to return before reranking

# EMBEDDER
EMBEDDER_MODEL_NAME=Qwen/Qwen3-Embedding-0.6B # or jinaai/jina-embeddings-v3 if you will
EMBEDDER_BASE_URL=http://vllm:8000/v1
EMBEDDER_API_KEY=EMPTY

# RERANKER
RERANKER_ENABLED=false
RERANKER_MODEL=Alibaba-NLP/gte-multilingual-reranker-base # or jinaai/jina-reranker-v2-base-multilingual or jinaai/jina-colbert-v2 if you want
RERANKER_BASE_URL=http://reranker:7997 # If using infinity
RERANKER_MODEL_TYPE=crossencoder # colbert or infinity
RERANKER_TOP_K=5 # Number of documents to return after reranking. upgrade to 8 for better results if your llm has a wider context window

# Prompts
PROMPTS_DIR=../prompts/example3

# Loaders
PDFLoader=MarkerLoader
XDG_CACHE_HOME=/app/model_weights
# If using MarkerLoader
MARKER_MAX_TASKS_PER_CHILD=100
MARKER_MAX_PROCESSES=10
MARKER_MIN_PROCESSES=3
MARKER_POOL_SIZE=3
MARKER_NUM_GPUS=0.6

# Audio
WHISPER_MODEL=base

# To enable API HTTP authentication via HTTPBearer
AUTH_TOKEN=sk-ragondin-1234

# set to true if you want to deploy chainlit ui
WITH_CHAINLIT_UI=true

# Ray
RAY_DEDUP_LOGS=0
RAY_DASHBOARD_PORT=8265

RAY_NUM_GPUS=0.1
RAY_POOL_SIZE=1 # Number of serializer actor instances
RAY_MAX_TASKS_PER_WORKER=5 # Number of tasks per serializer instance
RAY_DASHBOARD_PORT=8265
RAY_RUNTIME_ENV_HOOK=ray._private.runtime_env.uv_runtime_env_hook.hook

# SAVE UPLOADED FILES
SAVE_UPLOADED_FILES=true # usefull for chainlit source viewing


## Variable to add to activate indexer-ui
# INDEXERUI_COMPOSE_FILE=extern/indexer-ui/docker-compose.yaml  # Required path to the docker-compose file
# INDEXERUI_PORT=8067                                             # Port to expose the Indexer UI (default is 3042)
# INDEXERUI_URL='http://X.X.X.X:INDEXERUI_PORT'                   # Base URL of the Indexer UI (required to prevent CORS issues)
# VITE_API_BASE_URL='http://X.X.X.X:APP_PORT'                     # Base URL of your FastAPI backend. Used by the frondend



## Specific to Ray cluster
# SHARED_ENV=/ray_mount/.env
# MODEL_WEIGHTS_VOLUME=/ray_mount/model_weights
# DATA_VOLUME=/ray_mount/data
# CONFIG_VOLUME=/ray_mount/.hydra_config
# DB_VOLUME=/ray_mount/db
# UV_LINK_MODE=copy
# UV_CACHE_DIR=/tmp/uv-cache 
# RAY_ADDRESS=ray://X.X.X.X:10001
# HEAD_NODE_IP=X.X.X.X

# # Chainlit UI authentication
# CHAINLIT_AUTH_SECRET=... # has to be generated with with this command: 'uv run chainlit create-secret' but a random value works too.
# CHAINLIT_USERNAME=Ragondin
# CHAINLIT_PASSWORD=Ragondin2025

# # For chainlit data (chat history, etc) persistency
# ## Persistency services (localstack + AWS (Deployed Locally))
# CHAINLIT_DATALAYER_COMPOSE=extern/chainlit-datalayer/compose.yaml


# ## PostgreSQL instance for data persistency.
# POSTGRES_USER=root
# POSTGRES_PASSWORD=root
# POSTGRES_DB=postgres
# POSTGRES_PORT=5432
# DATABASE_URL=postgresql://${POSTGRES_USER:-root}:${POSTGRES_PASSWORD:-root}@postgres:${POSTGRES_PORT:-5432}/${POSTGRES_DB:-postgres} # for chainlit

# ## S3 configuration (This is bucket deploy in deploy in a container).
# BUCKET_NAME=my-bucket
# APP_AWS_ACCESS_KEY=random-key
# APP_AWS_SECRET_KEY=random-key
# APP_AWS_REGION=eu-central-1
# LOCALSTACK_PORT=4566
# DEV_AWS_ENDPOINT=http://localstack:${LOCALSTACK_PORT:-4566}