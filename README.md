# 🦫 OpenRag — The Open RAG Experimentation Playground

OpenRag is a lightweight, modular and extensible Retrieval-Augmented Generation (RAG) framework designed to explore and test advanced RAG techniques — 100% open source and focused on experimentation, not lock-in.

> Built by the Linagora, OpenRag offers a sovereign-by-design alternative to mainstream RAG stacks.

## Table of Contents
- [🦫 OpenRag — The Open RAG Experimentation Playground](#-openrag--the-open-rag-experimentation-playground)
- [Table of Contents](#table-of-contents)
- [🎯 Goals](#-goals)
- [✨ Key Features](#-key-features)
  - [Supported File Formats](#supported-file-formats)
  - [Chunking](#chunking)
  - [Indexing](#indexing)
  - [Document Retrieval & Reranking](#document-retrieval--reranking)
  - [🔌 OpenAI API Compatibility](#-openai-api-compatibility)
- [🚀 Getting Started](#-getting-started)
  - [Prerequisites](#prerequisites)
  - [Installation and Configuration](#installation-and-configuration)
  - [Configuration](#configuration)
- [🔧 Troubleshooting](#-troubleshooting)
- [🤝 Contributing](#-contributing)


## 🎯 Goals
- Experiment with advanced RAG techniques
- Develop evaluation metrics for RAG applications
- Collaborate with the community to innovate and push the boundaries of RAG applications

## ✨ Key Features
This section provides a detailed explanation of the currently supported features.

The **`.hydra_config`** directory contains all the configuration files for the application. These configurations are structured using the [Hydra configuration framework](https://hydra.cc/docs/intro/). This directory will be referenced for setting up the RAG (Retrieval-Augmented Generation) pipeline.

### Supported File Formats
This branch currently supports the following file types:

* **TextFiles**: `txt`, `md`
* **Document Files**: `pdf`, `docx`, `doc`, `pptx`
* **Audio Files**: `wav`, `mp3`, `mp4`, `ogg`, `flv`, `wma`, `aac`
* **Images**: `png:, jpeg, jpg, svg`

Files are converted tp **Markdown**, with images replaced by captions generated by a **Vision Language Model (VLM)**. (Refer to the **Configuration** section for additional details.) The final Markdown output is then split into chunks and indexed in the [Milvus vector database](https://milvus.io/).

>[!NOTE]
> **Upcoming Support**: Future releases will expand compatibility to include additional formats such as `csv`, `odt`, `html`, and other widely used open-source document types.

### Chunking
Multiple [chunking strategies](./.hydra_config/chunker) are supported: **`semantic`, `markdown`, and `recursive`** chunking. Files are converted to markdown and the **same chunker** is used for all types. Format-specific chunkers (e.g., for CSV, HTML) will be added later. 

```yml
# .hydra_config/chunker/markdown_splitter.yaml
defaults:
  - base
name: markdown_splitter
chunk_size: 512
chunk_overlap: 100
```

The **`chunk_size`** and **`chunk_overlap`** values are expressed in **tokens**, not characters. For enhanced retrieval, enable the **contextual retrieval** — a technique introduced by Anthropic to improve retrieval performance ([Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)).

### Indexing
Chunks are stored in the **Milvus** vector database using the `Qwen/Qwen3-Embedding-0.6B` embedder via VLLM. To explore alternatives, check the [MTEB benchmark](https://huggingface.co/spaces/mteb/leaderboard).

> \[!IMPORTANT]
> Use an embedding model suited to your document languages and context window needs. The default model supports English and French.


### Document Retrieval & Reranking
* Search Pipeline: We use a **hybrid search** combining **semantic search** and **BM25** keyword matching for broader coverage. Results are merged and ranked with [Reciprocal Rank Fusion (RRF)](https://milvus.io/docs/reranking.md) for optimal relevance.

> \[!IMPORTANT]
> Semantic similarity doesn't always mean relevance. Rerankers help refine results and reduce hallucinations by prioritizing the most relevant documents.

* *Reranker: Documents are then reranked using the multilingual reranker **`Alibaba-NLP/gte-multilingual-reranker-base`** model from Hugging Face.

### 🔌 OpenAI API Compatibility

Our RAG pipeline natively supports the **OpenAI API**, ensuring seamless integration with any tool or workflow built around the OpenAI interface. This means you can plug it directly into popular frontends like **`OpenWebUI`**—no custom adapters needed.

Looking to get started? Check out the [API documentation](docs/api_documentation.md#-openai-compatible-chat) for full details.



## 🚀 Getting Started

### Prerequisites
- **Python 3.12** or higher recommended
- **Docker** and **Docker Compose**
- For GPU capable machines, ensure you have the NVIDIA Container Toolkit installed. Refer to the [NVIDIA documentation](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html) for installation instructions.

OpenRag is designed to run in a containerized environment under Linux on x86_64 architecture. ARM processors are not supported, this is subject to change in the future.

### Installation and Configuration
#### 1. Clone the repository:
```bash
git clone git@github.com:linagora/openrag.git

# git clone --recurse-submodules git@github.com:linagora/openrag.git # to clone the repo with the associated submodules

cd openrag
git checkout main # or a given release
```

#### 2. Create uv environment and install dependencies:
>[!IMPORTANT] 
> Ensure you have Python 3.12 installed along with `uv`. For detailed installation instructions for uv, refer to the [uv official documentation](https://docs.astral.sh/uv/getting-started/installation/#pypi). You can either use `uv` or `pip` (if already available) or `curl`. Additional installation methods are outlined in the [documentation](https://docs.astral.sh/uv/getting-started/installation/#pypi).

```bash
# with pip
pip install uv

# with curl
curl -LsSf https://astral.sh/uv/install.sh | sh
```

```bash
# Create a new environment with all dependencies
cd openrag/
uv sync
```

#### 3. Create a `.env` File

Create a `.env` file at the root of the project, mirroring the structure of `.env.example`, to configure your environment.

###### File Parser configuration 
> For PDF indexing, multiple loader options are available. Set your choice using the **`PDFLoader`** env variable:
  * **`MarkerLoader`** and **`DoclingLoader`** are recommended for optimal performance, especially on OCR-processed PDFs. They support both GPU and CPU execution.
  * For lightweight testing on CPU, use **`PyMuPDF4LLMLoader`** or **`PyMuPDFLoader`**.
    > ⚠️ These do **not** support non-searchable PDFs or image-based content and images are not handled.

Other file formats (`txt`, `docx`, `doc`, `pptx`, audio type files, etc) are pre-configured.

```bash
# This is the minimal settings required.

# LLM
BASE_URL=
API_KEY=
MODEL=
LLM_SEMAPHORE=10

# VLM for image captioning. You can put your LLM here if it's multimodal 
VLM_BASE_URL=
VLM_API_KEY=
VLM_MODEL=
VLM_SEMAPHORE=10

# App
APP_PORT=8080 # forwarded port of the fastapi

# RETRIEVER
CONTEXTUAL_RETRIEVAL=true # see the `### Chunking` section
RETRIEVER_TOP_K=20 # Number of documents to return before reranking

# EMBEDDER
EMBEDDER_MODEL_NAME=Qwen/Qwen3-Embedding-0.6B
EMBEDDER_BASE_URL=http://vllm:8000/v1
EMBEDDER_API_KEY=EMPTY

# RERANKER
RERANKER_ENABLED=true
RERANKER_MODEL=Alibaba-NLP/gte-multilingual-reranker-base
RERANKER_TOP_K=5 # Number of documents to return after reranking. increment it for better results if your llm has a wider context window

# Prompts
PROMPTS_DIR=../prompts/example3_en # you can the fr version of the prompts ../prompts/example3

# Loaders
PDFLoader=MarkerLoader
MARKER_MAX_PROCESSES=2 # increment if you've enough gpu capacity

# RAY, to better understand the RAY Parameters, see section 5 on RAY
RAY_DEDUP_LOGS=0
RAY_NUM_GPUS=0.1
RAY_POOL_SIZE=1 # increment if you a cluster of machines
RAY_MAX_TASKS_PER_WORKER=5 # Number of tasks per serializer instance
RAY_DASHBOARD_PORT=8265
RAY_RUNTIME_ENV_HOOK=ray._private.runtime_env.uv_runtime_env_hook.hook

# To enable HTTP authentication via HTTPBearer for the api endpoints
AUTH_TOKEN=super-secret-token
```

###### Indexer UI
>[!IMPORTANT]
> Before launching the app, You might want to configure **`Indexer UI` (A Web interface for intuitive document ingestion, indexing, and management.)** following the dedicated guide:
➡ [Deploy with Indexer UI](docs/setup_indexerui.md)

#### 4.Deployment: Launch the app
You can run the application with either GPU or CPU support, depending on your system:

```bash
# Start with GPU (recommended for better performance)
docker compose up --build -d  # Use 'down' to stop

# Start with CPU
docker compose --profile cpu up --build -d # Use '--profile cpu down' to stop it properly
```
>[!TIPS]
> For quick testing on CPU, you can reduce computational load by adjusting the following settings in the **`.env`** file:

```bash
RERANKER_ENABLED=false # to disable ranking which is a costly operation
# If you want to keep then, reduce `RETRIEVER_TOP_K` to 10 or 20 to reduce ranking computation
```
>[!WARNING]
> These adjustments may affect performance and result quality but are appropriate for lightweight testing.

>[!INFO]
> The initial launch is longer due to the installation of required dependencies.

Once the app is up and running, visit `http://localhost:APP_PORT` or `http:X.X.X.X:APP_PORT` to access via:

1. **`/docs`** – FastAPI’s full API documentation. See [this guide](docs/api_documentation.md) for more details on the endpoints.
2. **`/chainlit`** – [Chainlit chat UI](https://docs.chainlit.io/get-started/overview) to chat with your partitions. To disable it (e.g., for backend-only use), set `WITH_CHAINLIT_UI=False`.

> \[!NOTE]
> Chainlit UI has no authentication by default. To enable it, set the following in your `.env`:

```bash
CHAINLIT_AUTH_SECRET=...       # Generate with: uv run chainlit create-secret (or use any random value)
CHAINLIT_USERNAME=Openrag
CHAINLIT_PASSWORD=Openrag2025
```
> \[!IMPORTANT]
> Chat history is **disabled** by default. To store conversations and related data, enable Chainlit’s data persistence layer.

➡ [Enable Chainlit Data Persistence](docs/chainlit_data_persistency.md)


3. If the **`Indexer UI`** (a web interface for easy document ingestion, indexing, and management) is enabled, you can access it at `http://localhost:INDEXERUI_PORT`. See [this guide](docs/setup_indexerui.md) for setup instructions.


#### 5. Distributed deployment in a Ray cluster

To scale **OpenRag** in a distributed environment using **Ray**, follow the dedicated guide:

➡ [Deploy OpenRag in a Ray cluster](docs/deploy_ray_cluster.md)

#### 6. 🧠 API Overview

This FastAPI-powered backend offers capabilities for document-based question answering (RAG), semantic search, and document indexing across multiple partitions. It exposes endpoints for interacting with a vector database and managing document ingestion, processing, and querying. See this document for [detailed overview of our api](docs/api_documentation.md).


## 🔧 Troubleshooting

### Error on dependencies installation

After running `uv sync`, if you have this error:

```
error: Distribution `ray==2.43.0 @ registry+https://pypi.org/simple` can't be installed because it doesn't have a source distribution or wheel for the current platform

hint: You're using CPython 3.13 (`cp313`), but `ray` (v2.43.0) only has wheels with the following Python ABI tag: `cp312`
```

This means your uv installation relies on cpython 3.13 while you are using python 3.12.

To solve it, please run:
```bash
uv venv --python=3.12
uv sync
```
### Error with models' weights downloading
While executing OpenRag, if you encounter a problem that prevents you from downloading the models' weights locally, then you just need to create the needed folder and authorize it to be written and executed

```bash
sudo mkdir /app/model_weights
sudo chmod 775 /app/model_weights
```

## 🤝 Contributing

We ❤️ contributions!

Contributions are welcome! Please follow standard GitHub workflow:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request